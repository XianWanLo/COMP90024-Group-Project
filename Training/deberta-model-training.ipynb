{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11758843,"sourceType":"datasetVersion","datasetId":7381856}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport torch\nimport json\n\n# Custom Dataset\nclass ClaimClassificationDataset(Dataset):\n    def __init__(self, data_path, tokenizer, max_len=512):\n        self.data = self.load_data(data_path)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n\n    def load_data(self, data_path):\n        \"\"\"Load and preprocess data from JSON.\"\"\"\n        with open(data_path, \"r\") as f:\n            data = json.load(f)\n        return data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        claim = \"CLAIM:\" + item['claim_text']\n        evidence = \" [SEP] EVIDENCE:\".join(item['evidences'])\n        #claim = item['claim_text']\n        #evidence = \" [SEP] \".join(item['evidences'])\n        inputs = self.tokenizer(claim + evidence,\n                                truncation=True, padding=\"max_length\",  # Use dynamic padding\n                                max_length=self.max_len, return_tensors='pt')\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n        inputs['labels'] = torch.tensor(self.label_map[item['claim_label']])\n        return inputs\n\ndef create_dataloaders(train_path, dev_path, tokenizer, batch_size=16, max_len=512):\n    \"\"\"\n    Creates training and validation data loaders.\n\n    Args:\n        train_path (str): Path to training data JSON file.\n        dev_path (str): Path to development data JSON file.\n        tokenizer (transformers.Tokenizer): BERT tokenizer.\n        batch_size (int): Batch size for data loaders.\n        max_len (int): Maximum length for tokenization.\n\n    Returns:\n        DataLoader: Training and validation data loaders.\n    \"\"\"\n    train_dataset = ClaimClassificationDataset(train_path, tokenizer, max_len)\n    dev_dataset = ClaimClassificationDataset(dev_path, tokenizer, max_len)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, dev_loader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.amp import GradScaler, autocast\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# GradScaler\nscaler = GradScaler()  \n\n   \ndef train_one_epoch(model, train_loader, optimizer, scheduler, autocast_flag=False):\n    model.train()\n    total_loss = 0\n\n    for batch in train_loader:\n        # Move data to device\n        # print(batch[\"input_ids\"])\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        if autocast_flag:\n            with autocast(device_type='cuda'):  # Updated autocast\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n        else:\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n\n         # Backward pass\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    return avg_loss\n\ndef evaluate(model, dev_loader):\n    model.eval()\n    total_loss = 0\n    correct_predictions = 0\n    total_samples = 0\n\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in dev_loader:\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            labels = batch[\"labels\"].to(DEVICE)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            # Compute accuracy\n            logits = outputs.logits\n            _, preds = torch.max(logits, dim=1)\n            correct_predictions += (preds == labels).sum().item()\n            total_samples += labels.size(0)\n\n            # Collect all predictions and labels for metrics calculation\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate average loss and accuracy\n    avg_loss = total_loss / len(dev_loader)\n    accuracy = correct_predictions / total_samples\n\n    # Calculate precision, recall, and F1 score\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n    # Confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    print(\"Confusion Matrix:\\n\", cm)\n    \n    # Classification report for per-class recall\n    # report = classification_report(all_labels, all_preds, zero_division=0)\n    # print(report)\n\n    # Return all metrics\n    metrics = {\n        \"avg_loss\": avg_loss,\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1\n    }\n\n    return metrics\n\ndef train_model(model, train_loader, dev_loader, epochs, optimizer, scheduler, best_model_path, autocast_flag):\n    best_accuracy = 0\n    best_precision = 100\n\n    print(\"START TRAINING: \")\n    for epoch in range(epochs):\n        print(f\"----Epoch {epoch + 1}/{epochs}----\")\n\n        # Training\n        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, autocast_flag)\n        print(f\"Training Loss: {train_loss:.4f}\")\n\n        # Evaluation\n        val_metrics = evaluate(model, dev_loader)\n        print(f\"Validation Loss:\", val_metrics)\n\n        # Save the best model\n        if val_metrics[\"accuracy\"] > best_accuracy:\n            best_accuracy = val_metrics[\"accuracy\"]\n            best_model = model\n            #torch.save(best_model, \"best_model.pt\")\n            #print(\"Model saved as 'best_model.pt'\")\n\n    best_model.to('cpu')\n    torch.save(best_model.state_dict(), best_model_path)\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nimport time \n\n# Define hyperparameters\nBATCH_SIZE = 4\nEPOCHS = 10\nMAX_LEN = 512\nLEARNING_RATE = 5e-5\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Paths\n# TRAIN_PATH = \"data/claim-evidence-set/train_set.json\"\n# DEV_PATH = \"data/claim-evidence-set/dev_set.json\"\nTRAIN_PATH = \"/kaggle/input/claim-evidence-pair/claim-evidence-set/claim-evidence-train_set.json\"\nDEV_PATH = \"/kaggle/input/claim-evidence-pair/claim-evidence-set/claim-evidence-dev_set.json\"\n\n# Load DeBERTa tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=4)\nmodel = model.to(DEVICE)\n\n# Create data loaders\ntrain_loader, dev_loader = create_dataloaders(TRAIN_PATH, DEV_PATH, tokenizer, batch_size=BATCH_SIZE, max_len=MAX_LEN)\n\n# Define optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_loader) * EPOCHS\n# Set warmup steps (10% of total steps)\nnum_warmup_steps = int(0.1 * total_steps)\n#num_warmup_steps = 0\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n\nbest_model_path = \"/kaggle/working/deBERTa_v3_best_model.pt\"\n\nif __name__ == \"__main__\":\n    start_time = time.time()\n    train_model(model, train_loader, dev_loader, EPOCHS, optimizer, scheduler, best_model_path, autocast_flag=False)\n    end_time = time.time()\n    print(\"Model training time:\", end_time-start_time)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}