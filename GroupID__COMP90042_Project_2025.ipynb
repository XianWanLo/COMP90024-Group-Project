{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32yCsRUo8H33"
   },
   "source": [
    "# 2025 COMP90042 Project #ellina\n",
    "*Make sure you change the file name with your group id.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCybYoGz8YWQ"
   },
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6po98qVA8bJD"
   },
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qvff21Hv8zjk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Earth', '’s', 'climate', 'sensitivity', 'is', 'important', '.']\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports for file handling, data types, and math\n",
    "import os, json, pickle, math\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Third-party libraries for NLP and retrieval\n",
    "# spaCy is used for tokenising text and filtering stopwords/punctuation\n",
    "# rank_bm25 provides the BM25 algorithm for evidence retrieval\n",
    "# sentence_transformers is used for SBERT encoding and cosine similarity\n",
    "import spacy                                # We use this for Tokenisation\n",
    "from rank_bm25 import BM25Okapi             # We use this for BM25 retrieval (Supposed to be better than TF-IDF)\n",
    "from sentence_transformers import SentenceTransformer, util  # SBERT encoding & cos-sim utils\n",
    "\n",
    "# File paths to the datasets\n",
    "TRAIN_CLAIMS_PATH  = \"data/train-claims.json\"\n",
    "DEV_CLAIMS_PATH    = \"data/dev-claims.json\"\n",
    "EVIDENCE_PATH      = \"data/evidence.json\"\n",
    "\n",
    "# Load a small English spaCy model for tokenisation\n",
    "nlp = spacy.load(\"en_core_web_sm\") # SpaCy model\n",
    "# Extra\n",
    "doc = nlp(\"The Earth’s climate sensitivity is important.\")\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# Load claims from the given JSON file\n",
    "def load_claims(claims_path: str) -> Dict:\n",
    "    \"\"\"Return claims from JSON file.\"\"\"\n",
    "    with open(claims_path) as f:\n",
    "        claims = json.load(f)\n",
    "    return claims\n",
    "\n",
    "# Load evidence passages from the given JSON file\n",
    "def load_evidences(evidence_path: str) -> Dict:\n",
    "    \"\"\"Return evidences from JSON file.\"\"\"\n",
    "    with open(evidence_path) as f:\n",
    "        evidences = json.load(f)\n",
    "    return evidences\n",
    "\n",
    "# Tokenise a list of texts using spaCy and save the result to a cache file.\n",
    "# If the cache already exists, load tokenised data directly from disk.\n",
    "# This speeds up repeated runs by avoiding re-processing the same data.\n",
    "def tokenise_cached(texts: List[str], cache_file: str) -> List[List[str]]:   \n",
    "    \"\"\"Simple spaCy tokenisation that caches to disk.\"\"\"\n",
    "    # Attempt to retrieve cached data \n",
    "    os.makedirs(os.path.dirname(cache_file), exist_ok=True)                  \n",
    "    if os.path.exists(cache_file):                                           \n",
    "        with open(cache_file, \"rb\") as f:                                    \n",
    "            return pickle.load(f)                                            \n",
    "\n",
    "    # Prepare to tokenise the list of documents and save to a file\n",
    "    out: List[List[str]] = []                                                \n",
    "    for doc in nlp.pipe(texts, batch_size=64):                               \n",
    "        tokens = [t.text for t in doc if not t.is_stop and not t.is_punct]   \n",
    "        out.append(tokens)                                                   \n",
    "    with open(cache_file, \"wb\") as f:                                        \n",
    "        pickle.dump(out, f)                                                  \n",
    "    return out  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA2ao2l8hOg"
   },
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Experiment: comparing results between TF-IDF vs BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  Precision   Recall  F1 Score\n",
      "TF-IDF        1.0 0.027027  0.052632\n",
      "  BM25        1.0 0.081081  0.150000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['evidence-107843', 'evidence-1140012', 'evidence-313382', 'evidence-336512', 'evidence-38305', 'evidence-397504', 'evidence-446603', 'evidence-459999', 'evidence-509525', 'evidence-616852', 'evidence-617077', 'evidence-65625', 'evidence-658407', 'evidence-668884', 'evidence-684667', 'evidence-726093', 'evidence-776422', 'evidence-78654', 'evidence-833215', 'evidence-848552', 'evidence-868602', 'evidence-875286', 'evidence-91026', 'evidence-917660', 'evidence-931093', 'evidence-945644', 'evidence-961268', 'evidence-962481', 'evidence-985452'] will be ignored\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['evidence-1011788', 'evidence-1026716', 'evidence-105435', 'evidence-107843', 'evidence-1096293', 'evidence-112298', 'evidence-1145317', 'evidence-1171953', 'evidence-1207394', 'evidence-147175', 'evidence-164501', 'evidence-364039', 'evidence-448761', 'evidence-519283', 'evidence-5590', 'evidence-566695', 'evidence-590326', 'evidence-603960', 'evidence-65625', 'evidence-683035', 'evidence-776422', 'evidence-875286', 'evidence-931093', 'evidence-942286', 'evidence-962481', 'evidence-985452', 'evidence-99302'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import math\n",
    "# from typing import Dict, List, Tuple\n",
    "# from itertools import islice\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# import pandas as pd\n",
    "\n",
    "# # === Load data ===\n",
    "# with open(\"data/dev-claims.json\") as f:\n",
    "#     dev_claims = json.load(f)\n",
    "# with open(\"data/evidence.json\") as f:\n",
    "#     evidence_data = json.load(f)\n",
    "\n",
    "# # === Use first X claims for testing ===\n",
    "# X = 10\n",
    "# claims = dict(islice(dev_claims.items(), X))\n",
    "# evidences = evidence_data  # Use full evidence set\n",
    "\n",
    "\n",
    "# # === Build gold evidence map ===\n",
    "# gold_map = {\n",
    "#     cid: list(map(str, data[\"evidences\"]))\n",
    "#     for cid, data in claims.items()\n",
    "#     if data[\"evidences\"]\n",
    "# }\n",
    "\n",
    "# # === TF-IDF retrieval ===\n",
    "# def tfidf_candidates(claims, evidences, top_k=3) -> Dict[str, List[str]]:\n",
    "#     claim_ids = list(claims)\n",
    "#     claim_texts = [claims[cid][\"claim_text\"] for cid in claim_ids]\n",
    "#     evidence_ids, evidence_texts = zip(*[(eid, txt) for eid, txt in evidences.items() if txt])\n",
    "\n",
    "#     all_texts = claim_texts + list(evidence_texts)\n",
    "#     vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "#     claim_vecs = tfidf_matrix[:len(claim_texts)]\n",
    "#     evidence_vecs = tfidf_matrix[len(claim_texts):]\n",
    "\n",
    "#     result = {}\n",
    "#     for i, cid in enumerate(claim_ids):\n",
    "#         sims = (claim_vecs[i] @ evidence_vecs.T).toarray().flatten()\n",
    "#         top_idx = sims.argsort()[::-1][:top_k]\n",
    "#         result[cid] = [evidence_ids[j] for j in top_idx]\n",
    "#     return result\n",
    "\n",
    "# # === BM25 retrieval ===\n",
    "# def bm25_candidates(claims, evidences, top_k=3) -> Dict[str, List[str]]:\n",
    "#     claim_ids = list(claims)\n",
    "#     claim_texts = [claims[cid][\"claim_text\"] for cid in claim_ids]\n",
    "#     evidence_ids, evidence_texts = zip(*[(eid, txt) for eid, txt in evidences.items() if txt])\n",
    "\n",
    "#     tok_e = [text.lower().split() for text in evidence_texts]\n",
    "#     tok_c = [text.lower().split() for text in claim_texts]\n",
    "\n",
    "#     bm25 = BM25Okapi(tok_e)\n",
    "\n",
    "#     result = {}\n",
    "#     for cid, toks in zip(claim_ids, tok_c):\n",
    "#         scores = bm25.get_scores(toks)\n",
    "#         top_idx = sorted(range(len(scores)), key=scores.__getitem__, reverse=True)[:top_k]\n",
    "#         result[cid] = [evidence_ids[i] for i in top_idx]\n",
    "#     return result\n",
    "\n",
    "# # === Evaluation ===\n",
    "# def evaluate(gold: Dict[str, List[str]], predicted: Dict[str, List[str]]):\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     y_true = mlb.fit_transform([gold[cid] for cid in gold])\n",
    "#     y_pred = mlb.transform([predicted.get(cid, []) for cid in gold])\n",
    "\n",
    "#     p = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "#     r = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "#     f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "#     return p, r, f1\n",
    "\n",
    "# # === Run retrieval and evaluate ===\n",
    "# tfidf_preds = tfidf_candidates(claims, evidences, top_k=3)\n",
    "# bm25_preds = bm25_candidates(claims, evidences, top_k=3)\n",
    "\n",
    "# # Filter claims that exist in all maps\n",
    "# common_ids = [cid for cid in gold_map if cid in tfidf_preds and cid in bm25_preds]\n",
    "# gold_eval = {cid: gold_map[cid] for cid in common_ids}\n",
    "# tfidf_eval = {cid: tfidf_preds[cid] for cid in common_ids}\n",
    "# bm25_eval = {cid: bm25_preds[cid] for cid in common_ids}\n",
    "\n",
    "# tfidf_scores = evaluate(gold_eval, tfidf_eval)\n",
    "# bm25_scores = evaluate(gold_eval, bm25_eval)\n",
    "\n",
    "# # === Show results ===\n",
    "# df = pd.DataFrame([\n",
    "#     {\"Method\": \"TF-IDF\", \"Precision\": tfidf_scores[0], \"Recall\": tfidf_scores[1], \"F1 Score\": tfidf_scores[2]},\n",
    "#     {\"Method\": \"BM25\", \"Precision\": bm25_scores[0], \"Recall\": bm25_scores[1], \"F1 Score\": bm25_scores[2]},\n",
    "# ])\n",
    "# print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QIEqDDT78q39"
   },
   "outputs": [],
   "source": [
    "# === BM25 filtering =================================================\n",
    "def bm25_candidates(\n",
    "    claims: Dict[str, dict], evidences: Dict[str, str],\n",
    "    top_k: int, ratio: float\n",
    ") -> Tuple[List[str], List[str], Dict[str, List[str]]]:\n",
    "    \"\"\"Return claim_ids, claim_texts, and BM25 top-k evidence IDs per claim.\"\"\"\n",
    "    claim_ids   = list(claims)\n",
    "    claim_texts = [claims[cid][\"claim_text\"] for cid in claim_ids]\n",
    "\n",
    "    evidence_ids, evidence_texts = zip(*[\n",
    "        (eid, txt) for eid, txt in evidences.items() if txt\n",
    "    ]) if evidences else ([], [])\n",
    "\n",
    "    tok_e = tokenise_cached(list(evidence_texts), f\"cache/evid_{len(evidence_texts)}.pkl\")\n",
    "    tok_c = tokenise_cached(claim_texts,          f\"cache/claim_{len(claim_texts)}.pkl\")\n",
    "\n",
    "    bm25 = BM25Okapi(tok_e)\n",
    "    k = top_k if len(evidence_ids) >= 500 else max(1, math.ceil(len(evidence_ids)*ratio))\n",
    "\n",
    "    cand_map: Dict[str, List[str]] = {}\n",
    "    for cid, toks in zip(claim_ids, tok_c):\n",
    "        scores  = bm25.get_scores(toks)\n",
    "        top_idx = sorted(range(len(scores)), key=scores.__getitem__, reverse=True)[:k]\n",
    "        cand_map[cid] = [evidence_ids[i] for i in top_idx]\n",
    "    return claim_ids, claim_texts, cand_map\n",
    "  \n",
    "# === Sentence-Bert re-ranking =================================================\n",
    "def sbert_rerank(\n",
    "    claim_ids: List[str], claim_texts: List[str], cand_map: Dict[str, List[str]],\n",
    "    evidences: Dict[str, str], model: SentenceTransformer, score_th: float\n",
    ") -> Dict[str, dict]:\n",
    "    \"\"\"Rerank BM25 candidates with SBERT cosine similarity.\"\"\"\n",
    "    results: Dict[str, dict] = {}\n",
    "\n",
    "    emb_claims = model.encode(claim_texts, convert_to_tensor=True)\n",
    "    idx_of = {cid: i for i, cid in enumerate(claim_ids)}\n",
    "\n",
    "    for cid in claim_ids:\n",
    "        c_vec = emb_claims[idx_of[cid]]\n",
    "        ev_scores = []\n",
    "        for eid in cand_map[cid]:\n",
    "            if eid not in evidences:\n",
    "                continue\n",
    "            e_vec = model.encode(evidences[eid], convert_to_tensor=True)\n",
    "            score = util.cos_sim(c_vec, e_vec).item()\n",
    "            ev_scores.append((eid, score))\n",
    "        kept = [p for p in ev_scores if p[1] >= score_th] or sorted(ev_scores, key=lambda x:x[1], reverse=True)[:1]\n",
    "        results[cid] = {\n",
    "            \"evidences\": [eid for eid, _ in kept],\n",
    "            \"scores\":    [round(s,4) for _, s in kept],\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzGuzHPE87Ya"
   },
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6ZVeNYIH9IaL"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# === Parameters ================================================\n",
    "BASE_MODEL_NAME    = \"all-MiniLM-L6-v2\"     # SBERT checkpoint (no fine-tune), TODO: Fine tune\n",
    "\n",
    "TOP_K_FIXED        = 100    # BM25 candidates per claim (upper bound)\n",
    "TOP_K_RATIO        = 0.20   # Ratio fallback when corpus is tiny\n",
    "SBERT_SCORE_TH     = 0.93   # Cosine-similarity threshold for sentence-bert\n",
    "\n",
    "LIMIT_DEV_CLAIMS   = True  # Quick-iteration switch, to limit the size of evidences for faster processing\n",
    "LIMIT_COUNT        = 100\n",
    "\n",
    "def evaluate(pred: dict, actual: dict):\n",
    "    gold_sets = [set(actual[c][\"evidences\"])          for c in actual]\n",
    "    pred_sets = [set(pred.get(c, {}).get(\"evidences\", [])) for c in actual]  \n",
    "\n",
    "    mlb = MultiLabelBinarizer()                     # ← turns set-of-IDs → multi-hot vector\n",
    "    y_true = mlb.fit_transform(gold_sets)\n",
    "    y_pred = mlb.transform(pred_sets)               # use same classes_\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"micro\", zero_division=0\n",
    "    )\n",
    "    return rec, prec, f1\n",
    "\n",
    "def main():\n",
    "    print(\"Loading datasets…\")\n",
    "    train_claims = load_claims(TRAIN_CLAIMS_PATH) # Todo: TRAIN S-BERT\n",
    "    development_claims   = load_claims(DEV_CLAIMS_PATH)\n",
    "    evidence_corpus = load_evidences(EVIDENCE_PATH)\n",
    "\n",
    "    if LIMIT_DEV_CLAIMS:\n",
    "        selected_claim_ids = list(development_claims)[:LIMIT_COUNT]\n",
    "        development_claims = {cid: development_claims[cid] for cid in selected_claim_ids}\n",
    "\n",
    "        # Gather only the evidence IDs actually referenced by the claims in the limited dev claims\n",
    "        required_evidence_ids = {\n",
    "            e_id for claim in development_claims.values() for e_id in claim[\"evidences\"]\n",
    "        }\n",
    "        evidence_corpus = {\n",
    "            e_id: evidence_corpus[e_id]                    # keep text\n",
    "            for e_id in required_evidence_ids\n",
    "            if e_id in evidence_corpus\n",
    "        }\n",
    "\n",
    "    # --- BM25 retrieval ---\n",
    "    print(\"BM25 candidate selection\")\n",
    "    candidate_ids, candidate_texts, candidate_map = bm25_candidates(\n",
    "        development_claims,\n",
    "        evidence_corpus,\n",
    "        TOP_K_FIXED,\n",
    "        TOP_K_RATIO\n",
    "    )\n",
    "\n",
    "    # candidate_ids – ascending order of claim IDs \n",
    "    # candidate_texts – parallel list of raw claim strings\n",
    "    # candidate_map – each claim mapped to its K best evidence IDs \n",
    "\n",
    "    # --- SBERT rerank ---\n",
    "    print(\"Loading SBERT model\")\n",
    "    sentence_bert = SentenceTransformer(BASE_MODEL_NAME)    # encodes claim & evidence texts\n",
    "\n",
    "    print(\"SBERT reranking\")\n",
    "    predictions = sbert_rerank(\n",
    "        candidate_ids,\n",
    "        candidate_texts,\n",
    "        candidate_map,\n",
    "        evidence_corpus,\n",
    "        sentence_bert,\n",
    "        SBERT_SCORE_TH\n",
    "    )\n",
    "    # --- evaluation ---\n",
    "    rec, prec, f1 = evaluate(predictions, development_claims)\n",
    "    print(f\"\\nRecall: {rec:.4f} Precision: {prec:.4f} F1: {f1:.4f}\")\n",
    "\n",
    "    # --- save file ---\n",
    "    with open(\"dev-claims-predictions.json\", \"w\") as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(\"Exported predictions to dev-claims-predictions.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mefSOe8eTmGP"
   },
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
