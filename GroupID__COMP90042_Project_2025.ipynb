{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "# import random\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses, util\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import faiss\n",
        "\n",
        "# === Parameters ===\n",
        "TRAIN_CLAIMS_PATH = \"data/train-claims.json\"\n",
        "DEV_CLAIMS_PATH = \"data/dev-claims.json\"\n",
        "EVIDENCE_PATH = \"data/evidence.json\"\n",
        "FINETUNED_MODEL_PATH = \"models/bge-finetuned\"\n",
        "BASE_MODEL_NAME = \"BAAI/bge-base-en-v1.5\"\n",
        "\n",
        "# === Parameters ===\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 1\n",
        "TOP_K = 10\n",
        "LIMIT_EVIDENCES = True\n",
        "SIMILARITY_THRESHOLD = 0.90 # Used for selecting evidences\n",
        "MAX_RESULTS = 5 # Min 1 evidence and max 5 evidences\n",
        "\n",
        "# === Loaders ===\n",
        "def load_claims(path: str) -> Dict:\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_evidences(path: str) -> Dict:\n",
        "    with open(path) as f:\n",
        "        return json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73xqTnDbTUP3"
      },
      "source": [
        "Claim-Evidence Loader for Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkhAafZmTOOM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "\n",
        "\n",
        "class DynamicEvidenceDataset(Dataset):\n",
        "    def __init__(self, eval_path, claim_path, evidence_path, tokenizer, max_len=512):\n",
        "        self.eval_data = self.load_data(eval_path)\n",
        "        self.claim_data = self.load_data(claim_path)\n",
        "        self.evidence_data = self.load_data(evidence_path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
        "\n",
        "    def load_data(self, path):\n",
        "        with open(path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claim_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim_id = list(self.eval_data.keys())[idx]\n",
        "        evidences = self.eval_data.get(claim_id, {}).get('evidences', [])\n",
        "\n",
        "        # Fetch claim text\n",
        "        claim_text = self.claim_data[claim_id]['claim_text']\n",
        "\n",
        "        # Fetch evidence texts\n",
        "        evidence_texts = [self.evidence_data.get(e_id, \"\") for e_id in evidences]\n",
        "        evidence = \" [SEP] \".join(evidence_texts)\n",
        "\n",
        "        # Construct input text\n",
        "        inputs = self.tokenizer(\"CLAIM: \" + claim_text + \" [SEP] EVIDENCE: \" + evidence,\n",
        "                                truncation=True, padding='max_length',\n",
        "                                max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        inputs['labels'] = self.label_map[self.claim_data[claim_id]['claim_label']]\n",
        "        return inputs\n",
        "\n",
        "\n",
        "def create_dataloader(eval_path, claim_path, evidence_path, tokenizer, batch_size=16, max_len=512):\n",
        "    dataset = DynamicEvidenceDataset(eval_path, claim_path, evidence_path, tokenizer, max_len)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dzlophCRteq"
      },
      "source": [
        "Evidence Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "def faiss_candidates(\n",
        "    claims: Dict[str, dict], evidences: Dict[str, str],\n",
        "    model: SentenceTransformer, top_k: int\n",
        ") -> Tuple[List[str], List[str], Dict[str, List[str]]]:\n",
        "    # Filter evidences to only those mentioned in the claims\n",
        "    if LIMIT_EVIDENCES:\n",
        "        used_evidence_ids = set()\n",
        "        for c in claims.values():\n",
        "            used_evidence_ids.update(c[\"evidences\"])\n",
        "\n",
        "        # Add 5,000 random evidence IDs that are not not already used\n",
        "        additional_ids = list(set(evidences.keys()) - used_evidence_ids)\n",
        "        additional_ids = additional_ids[:5000]\n",
        "\n",
        "        all_ids = list(used_evidence_ids) + additional_ids\n",
        "        all_ids = [eid for eid in all_ids if eid in evidences and evidences[eid]]\n",
        "\n",
        "        evidences = {eid: evidences[eid] for eid in all_ids}\n",
        "\n",
        "    claim_ids = list(claims)\n",
        "    claim_texts = [claims[cid][\"claim_text\"] for cid in claim_ids]\n",
        "    evidence_ids, evidence_texts = zip(*[(eid, txt) for eid, txt in evidences.items() if txt]) if evidences else ([], [])\n",
        "\n",
        "    emb_e = model.encode(evidence_texts, batch_size=64, show_progress_bar=True,\n",
        "                            convert_to_numpy=True, normalize_embeddings=True, device='cpu')\n",
        "    os.makedirs(\"cache\", exist_ok=True)\n",
        "\n",
        "    emb_c = model.encode(claim_texts, batch_size=64, show_progress_bar=True,\n",
        "                         convert_to_numpy=True, normalize_embeddings=True, device='cpu')\n",
        "\n",
        "    d = emb_e.shape[1]\n",
        "    index = faiss.IndexHNSWFlat(d, 32)\n",
        "    index.hnsw.efConstruction = 200\n",
        "    index.verbose = True\n",
        "\n",
        "    # Index creation\n",
        "    print(\"Training FAISS index\")\n",
        "    start = time.time()\n",
        "    # INFO: We can use indexes that require training (search faster but may be less accurate) or we can use indexes that don't need training.\n",
        "\n",
        "    # TODO: Can probably remove this\n",
        "    # train_sample = emb_e[np.random.choice(len(emb_e), size=10_000, replace=False)]\n",
        "    # index.train(train_sample)\n",
        "    print(f\"Index trained in {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Adding embeddings to FAISS index in batches\")\n",
        "\n",
        "    for i in range(0, len(emb_e), 100):\n",
        "        try:\n",
        "            batch = emb_e[i:i+100]\n",
        "            index.add(batch)\n",
        "            print(f\"Added {i + len(batch)} / {len(emb_e)} embeddings\", flush=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed at batch {i}: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"Finished adding all vectors in {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    print(\"Performing FAISS search (per claim)\")\n",
        "    start = time.time()\n",
        "\n",
        "    I_all = []\n",
        "\n",
        "    for emb in tqdm(emb_c, desc=\"Searching claims\"):\n",
        "        sim_scores, I = index.search(np.expand_dims(emb, axis=0), top_k)\n",
        "        paired = list(zip(I[0], sim_scores[0]))\n",
        "\n",
        "        # Filter by similarity threshold\n",
        "        filtered = [idx for idx, score in paired if score >= SIMILARITY_THRESHOLD]\n",
        "\n",
        "        # If nothing passes threshold, take the best one as we want at-least 1 evidence per claim\n",
        "        if not filtered:\n",
        "            filtered = [paired[0][0]] if paired else []\n",
        "\n",
        "        # Limit to max results (5 evidences)\n",
        "        filtered = filtered[:MAX_RESULTS]\n",
        "        I_all.append(filtered)\n",
        "\n",
        "    print(f\"Search done in {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    # Create dict structure for the results\n",
        "    cand_map: Dict[str, List[str]] = {\n",
        "        claim_ids[i]: [\n",
        "            evidence_ids[j] for j in I_all[i] if j < len(evidence_ids)\n",
        "        ] for i in range(len(claim_ids))\n",
        "    }\n",
        "\n",
        "    return claim_ids, claim_texts, cand_map\n",
        "\n",
        "def train_model(model: SentenceTransformer, dataloader, loss_func, output_path: str, epochs: int = 1):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\n🔁 Epoch {epoch + 1}/{epochs}\")\n",
        "        model.fit(\n",
        "            train_objectives=[(dataloader, loss_func)],\n",
        "            epochs=1,\n",
        "            warmup_steps=100,\n",
        "            output_path=output_path,\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "\n",
        "        model.save(output_path)\n",
        "        print(f\"✅ Finished training Epoch {epoch + 1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZjm2xrTRvcK"
      },
      "source": [
        "Claim Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Yy7j_SvRyTJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import time\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to('cuda')\n",
        "            attention_mask = batch['attention_mask'].to('cuda')\n",
        "            labels = batch['labels'].to('cuda')\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    return all_preds, all_labels, inference_time\n",
        "\n",
        "\n",
        "def run_evaluation(eval_path, claim_path, evidence_path, model, tokenizer,output_path, batch_size=16, max_len=512):\n",
        "\n",
        "    label_map = {0: 'SUPPORTS', 1: 'REFUTES', 2: 'NOT_ENOUGH_INFO', 3: 'DISPUTED'}\n",
        "\n",
        "    dataloader = create_dataloader(eval_path, claim_path, evidence_path, tokenizer, batch_size, max_len)\n",
        "    preds, labels, inference_time = evaluate_model(model, dataloader)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "    # Prepare output dictionary (key: claim_id)\n",
        "    output_data = {}\n",
        "    eval_data = json.load(open(eval_path))\n",
        "\n",
        "    for idx, claim_id in enumerate(eval_data.keys()):\n",
        "        output_data[claim_id] = {\n",
        "            \"evidences\": eval_data[claim_id][\"evidences\"],\n",
        "            \"claim_label\": label_map[int(preds[idx])]\n",
        "        }\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(output_data, f, indent=4)\n",
        "\n",
        "    print(f\"Predictions saved to {output_path}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Total Inference Time: {inference_time:.2f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDB7jiRIROKk"
      },
      "source": [
        "Evidence Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "def evaluate(pred: dict, actual: dict):\n",
        "    # For each claim, get the set of gold and predicted evidence IDs\n",
        "    gold_sets = [set(actual[c][\"evidences\"]) for c in actual]\n",
        "    pred_sets = [set(pred.get(c, {}).get(\"evidences\", [])) for c in actual]\n",
        "\n",
        "    # Fit the label binarizer on the gold evidence universe\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    y_true = mlb.fit_transform(gold_sets)\n",
        "\n",
        "    valid_labels = set(mlb.classes_)  # only evidence IDs that appear in gold\n",
        "\n",
        "    # Filter predictions to only include valid evidence IDs (safe for shared use)\n",
        "    pred_sets_filtered = [\n",
        "        [eid for eid in preds if eid in valid_labels]\n",
        "        for preds in pred_sets\n",
        "    ]\n",
        "    y_pred = mlb.transform(pred_sets_filtered)\n",
        "\n",
        "    # Micro-averaged precision/recall/F1 (shared evidences per-claim are handled naturally)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"micro\", zero_division=0\n",
        "    )\n",
        "\n",
        "    return rec, prec, f1\n",
        "\n",
        "def predictions(cand_map: Dict[str, List[str]]) -> Dict[str, dict]:\n",
        "    return {cid: {\"evidences\": eids} for cid, eids in cand_map.items()}\n",
        "\n",
        "def compute_recall_f1(model: SentenceTransformer, dev_claims_path: str, evidence_path: str, top_k: int = 10, save_path: str = None):\n",
        "    dev_claims = load_claims(dev_claims_path)\n",
        "    evidences = load_evidences(evidence_path)\n",
        "\n",
        "    _, _, cand_map = faiss_candidates(dev_claims, evidences, model, top_k)\n",
        "    pred = predictions(cand_map)\n",
        "\n",
        "    if save_path:\n",
        "        with open(save_path, \"w\") as f:\n",
        "            json.dump(pred, f, indent=2)\n",
        "        print(f\"Saved predictions to {save_path}\")\n",
        "\n",
        "    return evaluate(pred, dev_claims)\n",
        "\n",
        "# === Prepare training data ===\n",
        "print(\"Loading training data\")\n",
        "claims = load_claims(TRAIN_CLAIMS_PATH)\n",
        "print(\"Loading evidences data\")\n",
        "evidences = load_evidences(EVIDENCE_PATH)\n",
        "evidence_ids = list(evidences.keys())\n",
        "\n",
        "\n",
        "# Get training examples (generate both positive and negative examples)\n",
        "train_examples = []\n",
        "for cid, cdata in tqdm(claims.items(), desc=\"🔧 Building training examples\"):\n",
        "    claim_text = cdata[\"claim_text\"]\n",
        "    positive_eids = cdata[\"evidences\"]\n",
        "    positive_texts = [evidences[eid] for eid in positive_eids if eid in evidences]\n",
        "    if not positive_texts:\n",
        "        continue\n",
        "    for pos_text in positive_texts:\n",
        "        train_examples.append(InputExample(texts=[claim_text, pos_text]))\n",
        "\n",
        "print(\"Loading model.\")\n",
        "if os.path.exists(FINETUNED_MODEL_PATH):\n",
        "    print(f\"Continuing from: {FINETUNED_MODEL_PATH}\")\n",
        "    model = SentenceTransformer(FINETUNED_MODEL_PATH)\n",
        "else:\n",
        "    print(f\"Starting from base model: {BASE_MODEL_NAME}\")\n",
        "    model = SentenceTransformer(BASE_MODEL_NAME)\n",
        "\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
        "\n",
        "try:\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_model(model, train_dataloader, train_loss, FINETUNED_MODEL_PATH, epochs=EPOCHS)\n",
        "\n",
        "        model.save(FINETUNED_MODEL_PATH)\n",
        "        print(f\"\\nModel saved to: {FINETUNED_MODEL_PATH}\")\n",
        "\n",
        "        # Evaluate on dev set\n",
        "        recall, precision, f1 = compute_recall_f1(model, DEV_CLAIMS_PATH, EVIDENCE_PATH, top_k=TOP_K, save_path=\"MyPredictions\")\n",
        "        print(f\"[Epoch {epoch+1}] Recall@{TOP_K}: {recall:.4f}, Precision: {precision:.4f}, F1: {f1:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnqZtT1bRqsZ"
      },
      "source": [
        "Claim Classification with BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i0_24puQx-Z"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "EVAL_PATH = \"MyPredictions\"\n",
        "STATE_DICT_PATH = \"Pretrained_model/new_bert_model_Autocast_explicitMarker_LR5e05.pt\"\n",
        "OUTPUT_PATH = \"Prediction/bert_prediction.json\"\n",
        "\n",
        "batch_size = 16\n",
        "max_len = 512\n",
        "\n",
        "def load_model_and_tokenizer(state_dict_path):\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
        "    if state_dict_path:\n",
        "        state_dict = torch.load(state_dict_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "        print(\"successfully loaded finetuned BERT model\")\n",
        "    model.to('cuda')\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer(STATE_DICT_PATH)\n",
        "\n",
        "# Run evaluation\n",
        "run_evaluation(EVAL_PATH, DEV_CLAIMS_PATH, EVIDENCE_PATH, model, tokenizer,OUTPUT_PATH, batch_size, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Claim Classification with deBERTa-v3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "EVAL_PATH = \"MyPredictions\"\n",
        "STATE_DICT_PATH = \"Pretrained_model/deBERTa_v3_best_model.pt\"\n",
        "OUTPUT_PATH = \"Prediction/deBERTa_prediction.json\"\n",
        "\n",
        "batch_size = 4\n",
        "max_len = 512\n",
        "\n",
        "def load_model_and_tokenizer(state_dict_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=4)\n",
        "    if state_dict_path:\n",
        "        state_dict = torch.load(state_dict_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "        print(\"successfully loaded finetuned deBERTa model\")\n",
        "    model.to('cuda')\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer(STATE_DICT_PATH)\n",
        "\n",
        "# Run evaluation\n",
        "run_evaluation(EVAL_PATH, DEV_CLAIMS_PATH, EVIDENCE_PATH, model, tokenizer,OUTPUT_PATH, batch_size, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
