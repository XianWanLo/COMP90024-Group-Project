{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32yCsRUo8H33"
   },
   "source": [
    "# 2025 COMP90042 Project #test\n",
    "*Make sure you change the file name with your group id.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCybYoGz8YWQ"
   },
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6po98qVA8bJD"
   },
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvff21Hv8zjk"
   },
   "outputs": [],
   "source": [
    "# === Standard-library imports =================================================\n",
    "import os, json, pickle, math\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# === Third-party imports ======================================================\n",
    "import spacy                                # We use this for Tokenisation\n",
    "from rank_bm25 import BM25Okapi             # We use this for BM25 retrieval (Supposed to be better than TF-IDF)\n",
    "from sentence_transformers import SentenceTransformer, util  # SBERT encoding & cos-sim utils\n",
    "\n",
    "# === paths ================================================\n",
    "TRAIN_CLAIMS_PATH  = \"data/train-claims.json\"\n",
    "DEV_CLAIMS_PATH    = \"data/dev-claims.json\"\n",
    "EVIDENCE_PATH      = \"data/evidence.json\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # SpaCy model\n",
    "\n",
    "def load_claims(claims_path: str) -> Dict:\n",
    "    \"\"\"Return claims from JSON file.\"\"\"\n",
    "    with open(claims_path) as f:\n",
    "        claims = json.load(f)\n",
    "    return claims\n",
    "\n",
    "def load_evidences(evidence_path: str) -> Dict:\n",
    "    \"\"\"Return evidences from JSON file.\"\"\"\n",
    "    with open(evidence_path) as f:\n",
    "        evidences = json.load(f)\n",
    "    return evidences\n",
    "\n",
    "def tokenise_cached(texts: List[str], cache_file: str) -> List[List[str]]:   \n",
    "    \"\"\"Simple spaCy tokenisation that caches to disk.\"\"\"\n",
    "    # Attempt to retrieve cached data \n",
    "    os.makedirs(os.path.dirname(cache_file), exist_ok=True)                  \n",
    "    if os.path.exists(cache_file):                                           \n",
    "        with open(cache_file, \"rb\") as f:                                    \n",
    "            return pickle.load(f)                                            \n",
    "\n",
    "    # Prepare to tokenise the list of documents and save to a file\n",
    "    out: List[List[str]] = []                                                \n",
    "    for doc in nlp.pipe(texts, batch_size=64):                               \n",
    "        tokens = [t.text for t in doc if not t.is_stop and not t.is_punct]   \n",
    "        out.append(tokens)                                                   \n",
    "    with open(cache_file, \"wb\") as f:                                        \n",
    "        pickle.dump(out, f)                                                  \n",
    "    return out  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FA2ao2l8hOg"
   },
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIEqDDT78q39"
   },
   "outputs": [],
   "source": [
    "# === BM25 filtering =================================================\n",
    "def bm25_candidates(\n",
    "    claims: Dict[str, dict], evidences: Dict[str, str],\n",
    "    top_k: int, ratio: float\n",
    ") -> Tuple[List[str], List[str], Dict[str, List[str]]]:\n",
    "    \"\"\"Return claim_ids, claim_texts, and BM25 top-k evidence IDs per claim.\"\"\"\n",
    "    claim_ids   = list(claims)\n",
    "    claim_texts = [claims[cid][\"claim_text\"] for cid in claim_ids]\n",
    "\n",
    "    evidence_ids, evidence_texts = zip(*[\n",
    "        (eid, txt) for eid, txt in evidences.items() if txt\n",
    "    ]) if evidences else ([], [])\n",
    "\n",
    "    tok_e = tokenise_cached(list(evidence_texts), f\"cache/evid_{len(evidence_texts)}.pkl\")\n",
    "    tok_c = tokenise_cached(claim_texts,          f\"cache/claim_{len(claim_texts)}.pkl\")\n",
    "\n",
    "    bm25 = BM25Okapi(tok_e)\n",
    "    k = top_k if len(evidence_ids) >= 500 else max(1, math.ceil(len(evidence_ids)*ratio))\n",
    "\n",
    "    cand_map: Dict[str, List[str]] = {}\n",
    "    for cid, toks in zip(claim_ids, tok_c):\n",
    "        scores  = bm25.get_scores(toks)\n",
    "        top_idx = sorted(range(len(scores)), key=scores.__getitem__, reverse=True)[:k]\n",
    "        cand_map[cid] = [evidence_ids[i] for i in top_idx]\n",
    "    return claim_ids, claim_texts, cand_map\n",
    "  \n",
    "# === Sentence-Bert re-ranking =================================================\n",
    "def sbert_rerank(\n",
    "    claim_ids: List[str], claim_texts: List[str], cand_map: Dict[str, List[str]],\n",
    "    evidences: Dict[str, str], model: SentenceTransformer, score_th: float\n",
    ") -> Dict[str, dict]:\n",
    "    \"\"\"Rerank BM25 candidates with SBERT cosine similarity.\"\"\"\n",
    "    results: Dict[str, dict] = {}\n",
    "\n",
    "    emb_claims = model.encode(claim_texts, convert_to_tensor=True)\n",
    "    idx_of = {cid: i for i, cid in enumerate(claim_ids)}\n",
    "\n",
    "    for cid in claim_ids:\n",
    "        c_vec = emb_claims[idx_of[cid]]\n",
    "        ev_scores = []\n",
    "        for eid in cand_map[cid]:\n",
    "            if eid not in evidences:\n",
    "                continue\n",
    "            e_vec = model.encode(evidences[eid], convert_to_tensor=True)\n",
    "            score = util.cos_sim(c_vec, e_vec).item()\n",
    "            ev_scores.append((eid, score))\n",
    "        kept = [p for p in ev_scores if p[1] >= score_th] or sorted(ev_scores, key=lambda x:x[1], reverse=True)[:1]\n",
    "        results[cid] = {\n",
    "            \"evidences\": [eid for eid, _ in kept],\n",
    "            \"scores\":    [round(s,4) for _, s in kept],\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzGuzHPE87Ya"
   },
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZVeNYIH9IaL"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# === Parameters ================================================\n",
    "BASE_MODEL_NAME    = \"all-MiniLM-L6-v2\"     # SBERT checkpoint (no fine-tune), TODO: Fine tune\n",
    "\n",
    "TOP_K_FIXED        = 100    # BM25 candidates per claim (upper bound)\n",
    "TOP_K_RATIO        = 0.20   # Ratio fallback when corpus is tiny\n",
    "SBERT_SCORE_TH     = 0.93   # Cosine-similarity threshold for sentence-bert\n",
    "\n",
    "LIMIT_DEV_CLAIMS   = True  # Quick-iteration switch, to limit the size of evidences for faster processing\n",
    "LIMIT_COUNT        = 100\n",
    "\n",
    "def evaluate(pred: dict, actual: dict):\n",
    "    gold_sets = [set(actual[c][\"evidences\"])          for c in actual]\n",
    "    pred_sets = [set(pred.get(c, {}).get(\"evidences\", [])) for c in actual]  \n",
    "\n",
    "    mlb = MultiLabelBinarizer()                     # ← turns set-of-IDs → multi-hot vector\n",
    "    y_true = mlb.fit_transform(gold_sets)\n",
    "    y_pred = mlb.transform(pred_sets)               # use same classes_\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"micro\", zero_division=0\n",
    "    )\n",
    "    return rec, prec, f1\n",
    "\n",
    "def main():\n",
    "    print(\"Loading datasets…\")\n",
    "    train_claims = load_claims(TRAIN_CLAIMS_PATH) # Todo: TRAIN S-BERT\n",
    "    development_claims   = load_claims(DEV_CLAIMS_PATH)\n",
    "    evidence_corpus = load_evidences(EVIDENCE_PATH)\n",
    "\n",
    "    if LIMIT_DEV_CLAIMS:\n",
    "        selected_claim_ids = list(development_claims)[:LIMIT_COUNT]\n",
    "        development_claims = {cid: development_claims[cid] for cid in selected_claim_ids}\n",
    "\n",
    "        # Gather only the evidence IDs actually referenced by the claims in the limited dev claims\n",
    "        required_evidence_ids = {\n",
    "            e_id for claim in development_claims.values() for e_id in claim[\"evidences\"]\n",
    "        }\n",
    "        evidence_corpus = {\n",
    "            e_id: evidence_corpus[e_id]                    # keep text\n",
    "            for e_id in required_evidence_ids\n",
    "            if e_id in evidence_corpus\n",
    "        }\n",
    "\n",
    "    # --- BM25 retrieval ---\n",
    "    print(\"BM25 candidate selection\")\n",
    "    candidate_ids, candidate_texts, candidate_map = bm25_candidates(\n",
    "        development_claims,\n",
    "        evidence_corpus,\n",
    "        TOP_K_FIXED,\n",
    "        TOP_K_RATIO\n",
    "    )\n",
    "\n",
    "    # candidate_ids – ascending order of claim IDs \n",
    "    # candidate_texts – parallel list of raw claim strings\n",
    "    # candidate_map – each claim mapped to its K best evidence IDs \n",
    "\n",
    "    # --- SBERT rerank ---\n",
    "    print(\"Loading SBERT model\")\n",
    "    sentence_bert = SentenceTransformer(BASE_MODEL_NAME)    # encodes claim & evidence texts\n",
    "\n",
    "    print(\"SBERT reranking\")\n",
    "    predictions = sbert_rerank(\n",
    "        candidate_ids,\n",
    "        candidate_texts,\n",
    "        candidate_map,\n",
    "        evidence_corpus,\n",
    "        sentence_bert,\n",
    "        SBERT_SCORE_TH\n",
    "    )\n",
    "    # --- evaluation ---\n",
    "    rec, prec, f1 = evaluate(predictions, development_claims)\n",
    "    print(f\"\\nRecall: {rec:.4f} Precision: {prec:.4f} F1: {f1:.4f}\")\n",
    "\n",
    "    # --- save file ---\n",
    "    with open(\"dev-claims-predictions.json\", \"w\") as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(\"Exported predictions to dev-claims-predictions.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mefSOe8eTmGP"
   },
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
