{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11758843,"sourceType":"datasetVersion","datasetId":7381856},{"sourceId":11836603,"sourceType":"datasetVersion","datasetId":7436486},{"sourceId":11836688,"sourceType":"datasetVersion","datasetId":7436548},{"sourceId":11841256,"sourceType":"datasetVersion","datasetId":7439786},{"sourceId":399332,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":326844,"modelId":347742},{"sourceId":399417,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":326894,"modelId":347790}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport time\n\nclass DynamicEvidenceDataset(Dataset):\n    def __init__(self, eval_path, claim_path, evidence_path, tokenizer, max_len=512):\n        self.eval_data = self.load_data(eval_path)\n        self.claim_data = self.load_data(claim_path)\n        self.evidence_data = self.load_data(evidence_path)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n\n    def load_data(self, path):\n        with open(path, 'r') as f:\n            return json.load(f)\n\n    def __len__(self):\n        return len(self.claim_data)\n\n    def __getitem__(self, idx):\n        claim_id = list(self.eval_data.keys())[idx]\n        evidences = self.eval_data.get(claim_id, {}).get('evidences', [])\n\n        # Fetch claim text\n        claim_text = self.claim_data[claim_id]['claim_text']\n\n        # Fetch evidence texts\n        evidence_texts = [self.evidence_data.get(e_id, \"\") for e_id in evidences]\n        evidence = \" [SEP] \".join(evidence_texts)\n\n        # Construct input text\n        inputs = self.tokenizer(\"CLAIM: \" + claim_text + \" [SEP] EVIDENCE: \" + evidence,\n                                truncation=True, padding='max_length',\n                                max_length=self.max_len, return_tensors='pt')\n\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n        inputs['labels'] = self.label_map[self.claim_data[claim_id]['claim_label']]  \n        return inputs\n\n\ndef create_dataloader(eval_path, claim_path, evidence_path, tokenizer, batch_size=16, max_len=512):\n    dataset = DynamicEvidenceDataset(eval_path, claim_path, evidence_path, tokenizer, max_len)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    return dataloader\n\n\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    start_time = time.time()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to('cuda')\n            attention_mask = batch['attention_mask'].to('cuda')\n            labels = batch['labels'].to('cuda')\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            _, preds = torch.max(logits, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    end_time = time.time()\n    inference_time = end_time - start_time\n\n    return all_preds, all_labels, inference_time\n\n\ndef run_evaluation(eval_path, claim_path, evidence_path, model, tokenizer,output_path, batch_size=16, max_len=512):\n    \n    label_map = {0: 'SUPPORTS', 1: 'REFUTES', 2: 'NOT_ENOUGH_INFO', 3: 'DISPUTED'}\n    \n    dataloader = create_dataloader(eval_path, claim_path, evidence_path, tokenizer, batch_size, max_len)\n    preds, labels, inference_time = evaluate_model(model, dataloader)\n\n    # Calculate metrics\n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    f1 = f1_score(labels, preds, average='weighted')\n    accuracy = accuracy_score(labels, preds)\n    \n    # Prepare output dictionary (key: claim_id)\n    output_data = {}\n    eval_data = json.load(open(eval_path))\n\n    for idx, claim_id in enumerate(eval_data.keys()):\n        output_data[claim_id] = {\n            \"evidences\": eval_data[claim_id][\"evidences\"],\n            \"claim_label\": label_map[int(preds[idx])]\n        }\n\n    with open(output_path, 'w') as f:\n        json.dump(output_data, f, indent=4)\n\n    print(f\"Predictions saved to {output_path}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Total Inference Time: {inference_time:.2f} seconds\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:47:38.824470Z","iopub.execute_input":"2025-05-17T15:47:38.824956Z","iopub.status.idle":"2025-05-17T15:47:38.838831Z","shell.execute_reply.started":"2025-05-17T15:47:38.824930Z","shell.execute_reply":"2025-05-17T15:47:38.838293Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"BERT Model Evaluation","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\nstate_dict_path = \"/kaggle/input/bert_base_pretrained/pytorch/default/1/baseline_bert_model_Autocast_explicitMarker_LR5e05.pt\"\neval_path = \"/kaggle/input/evidence-prediction/MyPredictions\"\nclaim_path = \"/kaggle/input/dev-claims/dev-claims.json\"\nevidence_path = \"/kaggle/input/evidence/evidence.json\"\noutput_path = \"/kaggle/working/BERT_prediction.json\"\nbatch_size = 16\nmax_len = 512\n\ndef load_model_and_tokenizer(state_dict_path):\n    \"\"\" Load the pre-trained model and tokenizer from the directory. \"\"\"\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n    if state_dict_path:\n        state_dict = torch.load(state_dict_path)\n        model.load_state_dict(state_dict)\n        print(\"successfully loaded model\")\n    model.to('cuda')\n    return model, tokenizer\n\n\n# Load model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(state_dict_path)\n\n# Run evaluation\nrun_evaluation(eval_path, claim_path, evidence_path, model, tokenizer,output_path, batch_size, max_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:53:32.494508Z","iopub.execute_input":"2025-05-17T15:53:32.494794Z","iopub.status.idle":"2025-05-17T15:53:40.044825Z","shell.execute_reply.started":"2025-05-17T15:53:32.494772Z","shell.execute_reply":"2025-05-17T15:53:40.044215Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"successfully loaded model\nPredictions saved to /kaggle/working/BERT_prediction.json\nAccuracy: 0.4675\nPrecision: 0.3534\nRecall: 0.4675\nF1 Score: 0.4024\nTotal Inference Time: 4.70 seconds\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"DeBERTa Model Evaluation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nstate_dict_path = \"/kaggle/input/deberta-v3-pretrained/pytorch/default/1/deBERTa_v3_best_model.pt\"\neval_path = \"/kaggle/input/evidence-prediction/MyPredictions\"\nclaim_path = \"/kaggle/input/dev-claims/dev-claims.json\"\nevidence_path = \"/kaggle/input/evidence/evidence.json\"\noutput_path = \"/kaggle/working/deBERTa_prediction.json\"\nbatch_size = 4\nmax_len = 512\n\ndef load_model_and_tokenizer(state_dict_path):\n    \"\"\" Load the pre-trained model and tokenizer from the directory. \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n    model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=4)\n    if state_dict_path:\n        state_dict = torch.load(state_dict_path)\n        model.load_state_dict(state_dict)\n        print(\"successfully loaded model\")\n    model.to('cuda')\n    return model, tokenizer\n\n\n# Load model and tokenizer\nmodel, tokenizer = load_model_and_tokenizer(state_dict_path)\n\n# Run evaluation\nrun_evaluation(eval_path, claim_path, evidence_path, model, tokenizer,output_path, batch_size, max_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:53:45.419167Z","iopub.execute_input":"2025-05-17T15:53:45.419858Z","iopub.status.idle":"2025-05-17T15:53:58.225538Z","shell.execute_reply.started":"2025-05-17T15:53:45.419835Z","shell.execute_reply":"2025-05-17T15:53:58.224870Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"successfully loaded model\nPredictions saved to /kaggle/working/deBERTa_prediction.json\nAccuracy: 0.4481\nPrecision: 0.3525\nRecall: 0.4481\nF1 Score: 0.3858\nTotal Inference Time: 7.30 seconds\n","output_type":"stream"}],"execution_count":10}]}