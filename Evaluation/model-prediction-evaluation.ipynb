{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-05-17T23:54:22.048186Z","iopub.status.busy":"2025-05-17T23:54:22.047942Z","iopub.status.idle":"2025-05-17T23:54:27.261421Z","shell.execute_reply":"2025-05-17T23:54:27.260542Z","shell.execute_reply.started":"2025-05-17T23:54:22.048168Z"},"trusted":true},"outputs":[],"source":["import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import time\n","\n","class DynamicEvidenceDataset(Dataset):\n","    def __init__(self, eval_path, claim_path, evidence_path, tokenizer, max_len=512):\n","        self.eval_data = self.load_data(eval_path)\n","        self.claim_data = self.load_data(claim_path)\n","        self.evidence_data = self.load_data(evidence_path)\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n","\n","    def load_data(self, path):\n","        with open(path, 'r') as f:\n","            return json.load(f)\n","\n","    def __len__(self):\n","        return len(self.claim_data)\n","\n","    def __getitem__(self, idx):\n","        claim_id = list(self.eval_data.keys())[idx]\n","        evidences = self.eval_data.get(claim_id, {}).get('evidences', [])\n","\n","        # Fetch claim text\n","        claim_text = self.claim_data[claim_id]['claim_text']\n","\n","        # Fetch evidence texts\n","        evidence_texts = [self.evidence_data.get(e_id, \"\") for e_id in evidences]\n","        evidence = \" [SEP] \".join(evidence_texts)\n","\n","        # Construct input text\n","        inputs = self.tokenizer(\"CLAIM: \" + claim_text + \" [SEP] EVIDENCE: \" + evidence,\n","                                truncation=True, padding='max_length',\n","                                max_length=self.max_len, return_tensors='pt')\n","\n","        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n","        inputs['labels'] = self.label_map[self.claim_data[claim_id]['claim_label']]  \n","        return inputs\n","\n","\n","def create_dataloader(eval_path, claim_path, evidence_path, tokenizer, batch_size=16, max_len=512):\n","    dataset = DynamicEvidenceDataset(eval_path, claim_path, evidence_path, tokenizer, max_len)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    return dataloader\n","\n","\n","def evaluate_model(model, dataloader):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to('cuda')\n","            attention_mask = batch['attention_mask'].to('cuda')\n","            labels = batch['labels'].to('cuda')\n","\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            _, preds = torch.max(logits, dim=1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","\n","    return all_preds, all_labels, inference_time\n","\n","\n","def run_evaluation(eval_path, claim_path, evidence_path, model, tokenizer,output_path, batch_size=16, max_len=512):\n","    \n","    label_map = {0: 'SUPPORTS', 1: 'REFUTES', 2: 'NOT_ENOUGH_INFO', 3: 'DISPUTED'}\n","    \n","    dataloader = create_dataloader(eval_path, claim_path, evidence_path, tokenizer, batch_size, max_len)\n","    preds, labels, inference_time = evaluate_model(model, dataloader)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(labels, preds)\n","    precision = precision_score(labels, preds, average='weighted')\n","    recall = recall_score(labels, preds, average='weighted')\n","    f1 = f1_score(labels, preds, average='weighted')\n","    accuracy = accuracy_score(labels, preds)\n","    \n","    # Prepare output dictionary (key: claim_id)\n","    output_data = {}\n","    eval_data = json.load(open(eval_path))\n","\n","    for idx, claim_id in enumerate(eval_data.keys()):\n","        output_data[claim_id] = {\n","            \"evidences\": eval_data[claim_id][\"evidences\"],\n","            \"claim_label\": label_map[int(preds[idx])]\n","        }\n","\n","    with open(output_path, 'w') as f:\n","        json.dump(output_data, f, indent=4)\n","\n","    print(f\"Predictions saved to {output_path}\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    print(f\"Total Inference Time: {inference_time:.2f} seconds\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["BERT Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-17T23:54:27.262590Z","iopub.status.busy":"2025-05-17T23:54:27.262248Z","iopub.status.idle":"2025-05-17T23:55:04.895472Z","shell.execute_reply":"2025-05-17T23:55:04.894704Z","shell.execute_reply.started":"2025-05-17T23:54:27.262566Z"},"trusted":true},"outputs":[],"source":["from transformers import BertTokenizer, BertForSequenceClassification\n","\n","state_dict_path = \"/kaggle/input/bert-pretrained/pytorch/default/1/baseline_bert_model_Autocast_explicitMarker_LR5e05.pt\"\n","eval_path = \"/kaggle/input/evidence-prediction/MyPredictions\"\n","claim_path = \"/kaggle/input/dev-claims/dev-claims.json\"\n","evidence_path = \"/kaggle/input/evidence/evidence.json\"\n","output_path = \"/kaggle/working/new_BERT_prediction.json\"\n","batch_size = 16\n","max_len = 512\n","\n","def load_model_and_tokenizer(state_dict_path):\n","    \"\"\" Load the pre-trained model and tokenizer from the directory. \"\"\"\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n","    if state_dict_path:\n","        state_dict = torch.load(state_dict_path)\n","        model.load_state_dict(state_dict)\n","        print(\"successfully loaded model\")\n","    model.to('cuda')\n","    return model, tokenizer\n","\n","\n","# Load model and tokenizer\n","model, tokenizer = load_model_and_tokenizer(state_dict_path)\n","\n","# Run evaluation\n","run_evaluation(eval_path, claim_path, evidence_path, model, tokenizer,output_path, batch_size, max_len)"]},{"cell_type":"markdown","metadata":{},"source":["DeBERTa Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-05-17T23:55:04.898492Z","iopub.status.busy":"2025-05-17T23:55:04.898008Z","iopub.status.idle":"2025-05-17T23:55:27.305198Z","shell.execute_reply":"2025-05-17T23:55:27.304309Z","shell.execute_reply.started":"2025-05-17T23:55:04.898472Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","state_dict_path = \"/kaggle/input/deberta-v3-pretrained/pytorch/default/1/deBERTa_v3_best_model.pt\"\n","eval_path = \"/kaggle/input/evidence-prediction/MyPredictions\"\n","claim_path = \"/kaggle/input/dev-claims/dev-claims.json\"\n","evidence_path = \"/kaggle/input/evidence/evidence.json\"\n","output_path = \"/kaggle/working/deBERTa_prediction.json\"\n","batch_size = 4\n","max_len = 512\n","\n","def load_model_and_tokenizer(state_dict_path):\n","    \"\"\" Load the pre-trained model and tokenizer from the directory. \"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n","    model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=4)\n","    if state_dict_path:\n","        state_dict = torch.load(state_dict_path)\n","        model.load_state_dict(state_dict)\n","        print(\"successfully loaded model\")\n","    model.to('cuda')\n","    return model, tokenizer\n","\n","\n","# Load model and tokenizer\n","model, tokenizer = load_model_and_tokenizer(state_dict_path)\n","\n","# Run evaluation\n","run_evaluation(eval_path, claim_path, evidence_path, model, tokenizer,output_path, batch_size, max_len)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":7381856,"sourceId":11758843,"sourceType":"datasetVersion"},{"datasetId":7436486,"sourceId":11836603,"sourceType":"datasetVersion"},{"datasetId":7436548,"sourceId":11836688,"sourceType":"datasetVersion"},{"datasetId":7439786,"sourceId":11841256,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":347742,"modelInstanceId":326844,"sourceId":399332,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":347790,"modelInstanceId":326894,"sourceId":399417,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":347860,"modelInstanceId":326962,"sourceId":399503,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31041,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
