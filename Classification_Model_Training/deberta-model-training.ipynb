{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from torch.utils.data import Dataset\n","import torch\n","import json\n","\n","# Custom Dataset\n","class ClaimClassificationDataset(Dataset):\n","    def __init__(self, data_path, tokenizer, max_len=512):\n","        self.data = self.load_data(data_path)\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n","\n","    def load_data(self, data_path):\n","        with open(data_path, \"r\") as f:\n","            data = json.load(f)\n","        return data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        claim = \"CLAIM:\" + item['claim_text']\n","        evidence = \" [SEP] EVIDENCE:\".join(item['evidences'])\n","        inputs = self.tokenizer(claim + evidence,\n","                                truncation=True, padding=\"max_length\",  # Use dynamic padding\n","                                max_length=self.max_len, return_tensors='pt')\n","        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n","        inputs['labels'] = torch.tensor(self.label_map[item['claim_label']])\n","        return inputs\n","\n","def create_dataloaders(train_path, dev_path, tokenizer, batch_size=16, max_len=512):\n","    \n","    train_dataset = ClaimClassificationDataset(train_path, tokenizer, max_len)\n","    dev_dataset = ClaimClassificationDataset(dev_path, tokenizer, max_len)\n","\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, dev_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.amp import GradScaler, autocast\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# GradScaler\n","scaler = GradScaler()  \n","\n","   \n","def train_one_epoch(model, train_loader, optimizer, scheduler, autocast_flag=False):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in train_loader:\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        labels = batch[\"labels\"].to(DEVICE)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        if autocast_flag:\n","            with autocast(device_type='cuda'):  # Updated autocast\n","                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = outputs.loss\n","        else:\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","\n","         # Backward pass\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    return avg_loss\n","\n","def evaluate(model, dev_loader):\n","    model.eval()\n","    total_loss = 0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in dev_loader:\n","            input_ids = batch[\"input_ids\"].to(DEVICE)\n","            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","            labels = batch[\"labels\"].to(DEVICE)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            # Compute accuracy\n","            logits = outputs.logits\n","            _, preds = torch.max(logits, dim=1)\n","            correct_predictions += (preds == labels).sum().item()\n","            total_samples += labels.size(0)\n","\n","            # Collect all predictions and labels for metrics calculation\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Calculate average loss and accuracy\n","    avg_loss = total_loss / len(dev_loader)\n","    accuracy = correct_predictions / total_samples\n","\n","    # Calculate precision, recall, and F1 score\n","    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n","    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n","    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(all_labels, all_preds)\n","    print(\"Confusion Matrix:\\n\", cm)\n","\n","    # Return all metrics\n","    metrics = {\n","        \"avg_loss\": avg_loss,\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1_score\": f1\n","    }\n","\n","    return metrics\n","\n","def train_model(model, train_loader, dev_loader, epochs, optimizer, scheduler, best_model_path, autocast_flag):\n","    best_accuracy = 0\n","\n","    print(\"START TRAINING: \")\n","    for epoch in range(epochs):\n","        print(f\"----Epoch {epoch + 1}/{epochs}----\")\n","\n","        # Training\n","        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, autocast_flag)\n","        print(f\"Training Loss: {train_loss:.4f}\")\n","\n","        # Evaluation\n","        val_metrics = evaluate(model, dev_loader)\n","        print(f\"Validation Loss:\", val_metrics)\n","\n","        # Save the best model\n","        if val_metrics[\"accuracy\"] > best_accuracy:\n","            best_accuracy = val_metrics[\"accuracy\"]\n","            best_model = model\n","            \n","\n","    best_model.to('cpu')\n","    torch.save(best_model.state_dict(), best_model_path)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n","from torch.optim import AdamW\n","import time \n","\n","# Define hyperparameters\n","BATCH_SIZE = 4\n","EPOCHS = 10\n","MAX_LEN = 512\n","LEARNING_RATE = 5e-5\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","TRAIN_PATH = \"/kaggle/input/claim-evidence-pair/claim-evidence-set/claim-evidence-train_set.json\"\n","DEV_PATH = \"/kaggle/input/claim-evidence-pair/claim-evidence-set/claim-evidence-dev_set.json\"\n","\n","# Load DeBERTa tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=4)\n","model = model.to(DEVICE)\n","\n","# Create data loaders\n","train_loader, dev_loader = create_dataloaders(TRAIN_PATH, DEV_PATH, tokenizer, batch_size=BATCH_SIZE, max_len=MAX_LEN)\n","\n","# Define optimizer and learning rate scheduler\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","total_steps = len(train_loader) * EPOCHS\n","num_warmup_steps = int(0.1 * total_steps)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n","\n","best_model_path = \"/kaggle/working/deBERTa_v3_best_model.pt\"\n","\n","if __name__ == \"__main__\":\n","    start_time = time.time()\n","    train_model(model, train_loader, dev_loader, EPOCHS, optimizer, scheduler, best_model_path, autocast_flag=False)\n","    end_time = time.time()\n","    print(\"Model training time:\", end_time-start_time)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":7381856,"sourceId":11758843,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
