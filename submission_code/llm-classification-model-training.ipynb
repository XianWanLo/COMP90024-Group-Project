{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11758843,"sourceType":"datasetVersion","datasetId":7381856}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Zero-shot learning (Direct Prompt)","metadata":{"id":"IF5hWkmHT3f5"}},{"cell_type":"code","source":"# pip install accelerate\nimport time\nimport json\nimport torch\n\n\ndef zero_shot_direct_prompt_classification(claim_text, evidences, tokenizer, model, max_len=512 ):\n\n    prompt = (\n        \"Classify if the evidences SUPPORTS, REFUTES, have NOT_ENOUGH_INFO or DISPUTED regarding the claim. Examples are provided for you to understand the logic.\\n\"\n\n        f\"Claim: {claim_text}\\n\"\n        f\"Evidence: {' [SEP] '.join(evidences)}\\n\"\n    )\n\n    # Tokenize and generate output\n    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).input_ids.to(\"cuda\")\n    output_ids = model.generate(input_ids, max_length=10)\n    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    # print(output)\n\n    # Ensure the output is one of the valid labels\n    valid_labels = [\"SUPPORTS\", \"REFUTES\", \"NOT_ENOUGH_INFO\", \"DISPUTED\"]\n    output = output.strip().upper()\n\n    return output if output in valid_labels else \"NOT_ENOUGH_INFO\"","metadata":{"id":"DHFfFSg_Rqgz","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:05:02.728878Z","iopub.execute_input":"2025-05-17T09:05:02.729171Z","iopub.status.idle":"2025-05-17T09:05:02.734690Z","shell.execute_reply.started":"2025-05-17T09:05:02.729150Z","shell.execute_reply":"2025-05-17T09:05:02.733894Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Zero-shot learning (Role-play Prompt)","metadata":{"id":"36ZoWme1RtLH"}},{"cell_type":"code","source":"def zero_shot_roleplay_prompt_classification(claim_text, evidences, tokenizer, model, max_len=512):\n\n    prompt = (\n        \"You are a fact-checking assistant and your task is to classify if the evidences SUPPORTS, REFUTES, have NOT_ENOUGH_INFO or DISPUTED regarding the claim.\\n\"\n\n        f\"Claim: {claim_text}\\n\"\n        f\"Evidence: {' [SEP] '.join(evidences)}\\n\"\n    )\n\n    # Tokenize and generate output\n    input_ids = tokenizer(prompt, return_tensors=\"pt\",truncation=True, max_length=max_len).input_ids.to(\"cuda\")\n    output_ids = model.generate(input_ids, max_length=10)\n    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    # print(output)\n\n    # Ensure the output is one of the valid labels\n    valid_labels = [\"SUPPORTS\", \"REFUTES\", \"NOT_ENOUGH_INFO\", \"DISPUTED\"]\n    output = output.strip().upper()\n\n    return output if output in valid_labels else \"NOT_ENOUGH_INFO\"","metadata":{"id":"yOJhiKvXRrqy","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:05:02.735807Z","iopub.execute_input":"2025-05-17T09:05:02.736067Z","iopub.status.idle":"2025-05-17T09:05:02.751604Z","shell.execute_reply.started":"2025-05-17T09:05:02.736047Z","shell.execute_reply":"2025-05-17T09:05:02.751004Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Few-shot Learning (Direct Prompt)","metadata":{"id":"9NryQj_EU_2K"}},{"cell_type":"code","source":"def few_shot_direct_prompt_classification(claim_text, evidences, tokenizer, model, max_len=512 ):\n\n    prompt = (\n         \"Classify if the evidences SUPPORTS, REFUTES, have NOT_ENOUGH_INFO or DISPUTED regarding the claim. Examples are provided below for reference.\\n\"\n\n        \"Example 1:\\n\"\n        \"Claim: \\\"Our harmless emissions of trifling quantities of carbon dioxide cannot possibly acidify the oceans.\\\"\\n\"\n        \"Evidence: \\\"Carbon dioxide also causes ocean acidification because it dissolves in water to form carbonic acid.\\\", \"\n        \"\\\"Marine calcifiers exhibit mixed responses to CO 2-induced ocean acidification.\\\"\\n\"\n        \"Answer: REFUTES\\n\\n\"\n\n        \"Example 2:\\n\"\n        \"Claim: \\\"Sea-level rise does not seem to depend on ocean temperature, and certainly not on CO2\\\"\\n\"\n        \"Evidence: \\\"This depth depends on (among other things) temperature and the amount of CO 2 dissolved in the ocean.\\\", \"\n        \"\\\"Because different climate models have slightly different patterns of ocean heating, they do not agree fully on the predictions for the contribution of ocean heating on sea level rise.\\\"\\n\"\n        \"Answer: DISPUTED\\n\\n\"\n\n        \"Example 3:\\n\"\n        \"Claim: \\\"Ocean and surface temperature measurements find the planet continues to accumulate heat.\\\"\\n\"\n        \"Evidence: \\\"Greenhouse gases trap heat radiating from the Earth to space.\\\", \"\n         \"\\\"Energy from the Sun heats this layer, and the surface below, causing expansion of the air.\\\"\\n\"\n        \"Answer: NOT_ENOUGH_INFO\\n\\n\"\n\n         \"Now classify the following claim:\\n\"\n        f\"Claim: {claim_text}\\n\"\n        f\"Evidence: {' [SEP] '.join(evidences)}\\n\"\n        \"Answer: \"\n    )\n\n    # Tokenize and generate output\n    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).input_ids.to(\"cuda\")\n    output_ids = model.generate(input_ids, max_length=10)\n    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    # print(output)\n\n    # Ensure the output is one of the valid labels\n    valid_labels = [\"SUPPORTS\", \"REFUTES\", \"NOT_ENOUGH_INFO\", \"DISPUTED\"]\n    output = output.strip().upper()\n\n    return output if output in valid_labels else \"NOT_ENOUGH_INFO\"","metadata":{"id":"DJzR7q-qRsDP","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:05:02.752305Z","iopub.execute_input":"2025-05-17T09:05:02.752514Z","iopub.status.idle":"2025-05-17T09:05:02.764442Z","shell.execute_reply.started":"2025-05-17T09:05:02.752492Z","shell.execute_reply":"2025-05-17T09:05:02.763723Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"Few-shot Learning (Role Play Prompt)","metadata":{"id":"yG3uMT-8XHtG"}},{"cell_type":"code","source":"def few_shot_roleplay_prompt_classification(claim_text, evidences, tokenizer, model, max_len=512):\n\n    prompt = (\n         \"You are a fact-checking assistant and your task is to classify if the evidences SUPPORTS, REFUTES, have NOT_ENOUGH_INFO or DISPUTED regarding the claim. Examples are provided below for reference.\\n\"\n\n        \"Example 1:\\n\"\n        \"Claim: \\\"Our harmless emissions of trifling quantities of carbon dioxide cannot possibly acidify the oceans.\\\"\\n\"\n        \"Evidence: \\\"Carbon dioxide also causes ocean acidification because it dissolves in water to form carbonic acid.\\\", \"\n        \"\\\"Marine calcifiers exhibit mixed responses to CO 2-induced ocean acidification.\\\"\\n\"\n        \"Answer: REFUTES\\n\\n\"\n\n        \"Example 2:\\n\"\n        \"Claim: \\\"Sea-level rise does not seem to depend on ocean temperature, and certainly not on CO2\\\"\\n\"\n        \"Evidence: \\\"This depth depends on (among other things) temperature and the amount of CO 2 dissolved in the ocean.\\\", \"\n        \"\\\"Because different climate models have slightly different patterns of ocean heating, they do not agree fully on the predictions for the contribution of ocean heating on sea level rise.\\\"\\n\"\n        \"Answer: DISPUTED\\n\\n\"\n\n        \"Example 3:\\n\"\n        \"Claim: \\\"Ocean and surface temperature measurements find the planet continues to accumulate heat.\\\"\\n\"\n        \"Evidence: \\\"Greenhouse gases trap heat radiating from the Earth to space.\\\", \"\n         \"\\\"Energy from the Sun heats this layer, and the surface below, causing expansion of the air.\\\"\\n\"\n        \"Answer: NOT_ENOUGH_INFO\\n\\n\"\n\n         \"Now classify the following claim:\\n\"\n        f\"Claim: {claim_text}\\n\"\n        f\"Evidence: {' [SEP] '.join(evidences)}\\n\"\n        \"Answer: \"\n    )\n\n    # Tokenize and generate output\n    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).input_ids.to(\"cuda\")\n    output_ids = model.generate(input_ids, max_length=10)\n    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    # print(output)\n\n    # Ensure the output is one of the valid labels\n    valid_labels = [\"SUPPORTS\", \"REFUTES\", \"NOT_ENOUGH_INFO\", \"DISPUTED\"]\n    output = output.strip().upper()\n\n    return output if output in valid_labels else \"NOT_ENOUGH_INFO\"","metadata":{"id":"Ngk3dilGRsWO","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:05:02.765577Z","iopub.execute_input":"2025-05-17T09:05:02.765777Z","iopub.status.idle":"2025-05-17T09:05:02.780132Z","shell.execute_reply.started":"2025-05-17T09:05:02.765762Z","shell.execute_reply":"2025-05-17T09:05:02.779403Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"LLM Pipeline","metadata":{"id":"fU8ANaIJXn_C"}},{"cell_type":"code","source":"# pip install accelerate\nimport time\nimport json\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n\ndef evaluate_llm(dev_data, mode, tokenizer, model ):\n\n    start_time = time.time()\n\n    # Define the label map to ensure consistency\n    label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n\n    # Initialize lists for storing ground truth and predictions\n    ground_truths = []\n    predictions = []\n\n    for item in dev_data:\n        claim_id = item[\"claim_id\"]\n        claim_text = item[\"claim_text\"]\n        evidences = item[\"evidences\"]\n        true_label = item[\"claim_label\"]\n\n        # zero-shot direct prompting\n        if mode == \"zero-shot-direct\":\n          predicted_label = zero_shot_direct_prompt_classification(claim_text, evidences, tokenizer, model )\n\n        # zero-shot role play prompting\n        if mode == \"zero-shot-role-play\":\n          predicted_label = zero_shot_roleplay_prompt_classification(claim_text, evidences, tokenizer, model )\n\n        # few-shot direct prompting\n        if mode == \"few-shot-direct\":\n          predicted_label = few_shot_direct_prompt_classification(claim_text, evidences, tokenizer, model )\n\n        # few-shot role play prompting\n        if mode == \"few-shot-role-play\":\n          predicted_label = few_shot_roleplay_prompt_classification(claim_text, evidences, tokenizer, model )\n\n        # Convert labels to integers using the label map\n        ground_truths.append(label_map[true_label])\n        predictions.append(label_map[predicted_label])\n\n    end_time = time.time()\n    total_inference_time = end_time - start_time\n\n    # Calculate metrics\n    accuracy = accuracy_score(ground_truths, predictions)\n    precision = precision_score(ground_truths, predictions, average='weighted', zero_division=0)\n    recall = recall_score(ground_truths, predictions, average='weighted', zero_division=0)\n    f1 = f1_score(ground_truths, predictions, average='weighted', zero_division=0)\n\n    #  # Confusion matrix\n    cm = confusion_matrix(ground_truths, predictions)\n    print(\"Confusion Matrix:\\n\", cm)\n\n    # Print metrics\n    print(f\"Total Inference Time: {total_inference_time:.2f} seconds\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n\n    # Return results and metrics\n    return {\n        \"inference_time\": total_inference_time,\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1\n    }\n\n# Evaluate\nwith open('/kaggle/input/claim-evidence-pair/claim-evidence-set/claim-evidence-dev_set.json', 'r') as f:\n  dev_data = json.load(f)\n\nprompting_mode = [\"zero-shot-direct\", \"zero-shot-role-play\", \"few-shot-direct\", \"few-shot-role-play\"]\n\nfor mode in prompting_mode:\n  print(f\"Prompting Mode: {mode}\")\n  # Load model and tokenizer\n  tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n  model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n  results = evaluate_llm(dev_data, mode, tokenizer, model )\n  print(results)\n  print(\"-----------------------------------------------------------\")","metadata":{"id":"VQ55moobRY0Z","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T09:05:02.922378Z","iopub.execute_input":"2025-05-17T09:05:02.922776Z","iopub.status.idle":"2025-05-17T09:08:19.617713Z","shell.execute_reply.started":"2025-05-17T09:05:02.922759Z","shell.execute_reply":"2025-05-17T09:08:19.617090Z"}},"outputs":[{"name":"stdout","text":"Prompting Mode: zero-shot-direct\nTotal Inference Time: 38.98 seconds\nAccuracy: 0.4740\nPrecision: 0.3139\nRecall: 0.4740\nF1 Score: 0.3579\n{'inference_time': 38.97865152359009, 'accuracy': 0.474025974025974, 'precision': 0.31387596360858394, 'recall': 0.474025974025974, 'f1_score': 0.357874852420307}\n-----------------------------------------------------------\nPrompting Mode: zero-shot-role-play\nTotal Inference Time: 39.65 seconds\nAccuracy: 0.5000\nPrecision: 0.3245\nRecall: 0.5000\nF1 Score: 0.3907\n{'inference_time': 39.64689254760742, 'accuracy': 0.5, 'precision': 0.3245380870519914, 'recall': 0.5, 'f1_score': 0.3907494304795654}\n-----------------------------------------------------------\nPrompting Mode: few-shot-direct\nTotal Inference Time: 54.63 seconds\nAccuracy: 0.4416\nPrecision: 0.5823\nRecall: 0.4416\nF1 Score: 0.4250\n{'inference_time': 54.62656545639038, 'accuracy': 0.44155844155844154, 'precision': 0.5822838014698479, 'recall': 0.44155844155844154, 'f1_score': 0.4249629470556551}\n-----------------------------------------------------------\nPrompting Mode: few-shot-role-play\nTotal Inference Time: 54.66 seconds\nAccuracy: 0.4416\nPrecision: 0.5090\nRecall: 0.4416\nF1 Score: 0.4007\n{'inference_time': 54.65888953208923, 'accuracy': 0.44155844155844154, 'precision': 0.509022930507667, 'recall': 0.44155844155844154, 'f1_score': 0.4006823365313931}\n-----------------------------------------------------------\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"RUN LOG","metadata":{}},{"cell_type":"markdown","source":"43.1s 17 Prompting Mode: zero-shot-direct\n44.3s 18 You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n111.0s 19 Confusion Matrix:\n111.0s 20 [[63  3  0  2]\n111.0s 21 [16 10  0  1]\n111.0s 22 [37  2  0  2]\n111.0s 23 [16  2  0  0]]\n111.0s 24 Total Inference Time: 41.72 seconds\n111.0s 25 Accuracy: 0.4740\n111.0s 26 Precision: 0.3139\n111.0s 27 Recall: 0.4740\n111.0s 28 F1 Score: 0.3579\n111.0s 29 {'inference_time': 41.7239887714386, 'accuracy': 0.474025974025974, 'precision': 0.31387596360858394, 'recall': 0.474025974025974, 'f1_score': 0.357874852420307}\n111.0s 30 -----------------------------------------------------------\n111.0s 31 Prompting Mode: zero-shot-role-play\n154.0s 32 Confusion Matrix:\n154.0s 33 [[60  6  0  2]\n154.0s 34 [10 17  0  0]\n154.0s 35 [32  4  0  5]\n154.0s 36 [14  4  0  0]]\n154.0s 37 Total Inference Time: 40.57 seconds\n154.0s 38 Accuracy: 0.5000\n154.0s 39 Precision: 0.3245\n154.0s 40 Recall: 0.5000\n154.0s 41 F1 Score: 0.3907\n154.0s 42 {'inference_time': 40.57264232635498, 'accuracy': 0.5, 'precision': 0.3245380870519914, 'recall': 0.5, 'f1_score': 0.3907494304795654}\n154.0s 43 -----------------------------------------------------------\n154.0s 44 Prompting Mode: few-shot-direct\n209.4s 45 Confusion Matrix:\n209.4s 46 [[55  1  2 10]\n209.4s 47 [ 7  3  1 16]\n209.4s 48 [11  0  6 24]\n209.4s 49 [13  0  1  4]]\n209.4s 50 Total Inference Time: 52.84 seconds\n209.4s 51 Accuracy: 0.4416\n209.4s 52 Precision: 0.5823\n209.4s 53 Recall: 0.4416\n209.4s 54 F1 Score: 0.4250\n209.4s 55 {'inference_time': 52.83831024169922, 'accuracy': 0.44155844155844154, 'precision': 0.5822838014698479, 'recall': 0.44155844155844154, 'f1_score': 0.4249629470556551}\n209.4s 56 -----------------------------------------------------------\n209.4s 57 Prompting Mode: few-shot-role-play\n264.4s 58 Confusion Matrix:\n264.4s 59 [[56  0  1 11]\n264.4s 60 [ 7  2  6 12]\n264.4s 61 [16  1  5 19]\n264.4s 62 [13  0  0  5]]\n264.4s 63 Total Inference Time: 53.10 seconds\n264.4s 64 Accuracy: 0.4416\n264.4s 65 Precision: 0.5090\n264.4s 66 Recall: 0.4416\n264.4s 67 F1 Score: 0.4007\n264.4s 68 {'inference_time': 53.097389459609985, 'accuracy': 0.44155844155844154, 'precision': 0.509022930507667, 'recall': 0.44155844155844154, 'f1_score': 0.4006823365313931}\n264.4s 69 -----------------------------------------------------------","metadata":{}}]}