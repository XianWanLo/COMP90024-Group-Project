{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "\n",
        "We perform **individual model training** on both evidence retrieval and classification task. All trained model is saved under Pretrained_Model directory.\n",
        "- The evidence retrieval model (Sentence Transformer) training script & log is saved in evidence_retrieval_model_training.py file.\n",
        "- All classification models(T5, BERT, deBERTa) training script & log is saved under /Classification_Model_Training directory \n",
        "\n",
        "To execute the **overall processing pipeline**, simply run this file to generate the prediction files under /Evaluation directory.\n",
        "- new_BERT_prediction.json: prediction generated with BERT model\n",
        "- deBERTa_prediction.json: prediction generated with deBERTa model\n",
        "\n",
        "Finally, to **evaluate the prediction file**, run below command:\n",
        "\n",
        "'''\n",
        "cd Evaluation  \n",
        "python eval.py --predictions ./BERT_prediction.json --groundtruth ../data/dev-claims.json  \n",
        "python eval.py --predictions ./deBERTa_prediction.json --groundtruth ../data/dev-claims.json  \n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P95sC7-WNNw"
      },
      "source": [
        "Basic Loader for Claims and Evidences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# import random\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import faiss\n",
        "\n",
        "# === Parameters ===\n",
        "TRAIN_CLAIMS_PATH = \"data/train-claims.json\"\n",
        "DEV_CLAIMS_PATH = \"data/dev-claims.json\"\n",
        "EVIDENCE_PATH = \"data/evidence.json\"\n",
        "FINETUNED_MODEL_PATH = \"Pretrained_Model/bge-finetuned\"\n",
        "BASE_MODEL_NAME = \"BAAI/bge-base-en-v1.5\"\n",
        "PREDICTIONS_SAVE_PATH = \"Evidence_Prediction/MyPredictions\"\n",
        "\n",
        "# === Parameters ===\n",
        "BATCH_SIZE = 16\n",
        "TOP_K = 5 # Min 1 evidence and max 5 evidences\n",
        "SIMILARITY_THRESHOLD = 0.90 # Used for selecting evidences\n",
        "\n",
        "# === Loaders ===\n",
        "def load_claims(path: str) -> Dict:\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_evidences(path: str) -> Dict:\n",
        "    with open(path) as f:\n",
        "        return json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73xqTnDbTUP3"
      },
      "source": [
        "Claim-Predicted Evidence Loader for Classification Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkhAafZmTOOM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "\n",
        "# Combined predicted evidence ID, dev-claim and evidence to construct data loader for classification task\n",
        "class DynamicEvidenceDataset(Dataset):\n",
        "    def __init__(self, predicted_evidence_path, claim_path, evidence_path, tokenizer, max_len=512):\n",
        "        self.predicted_evidence_ID = self.load_data(predicted_evidence_path)\n",
        "        self.claim_data = self.load_data(claim_path)\n",
        "        self.evidence_data = self.load_data(evidence_path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_map = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
        "\n",
        "    def load_data(self, path):\n",
        "        with open(path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.claim_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        claim_id = list(self.predicted_evidence_ID.keys())[idx]\n",
        "        pred_evidences_ID = self.predicted_evidence_ID.get(claim_id, {}).get('evidences', [])\n",
        "\n",
        "        # Fetch claim text\n",
        "        claim_text = self.claim_data[claim_id]['claim_text']\n",
        "\n",
        "        # Fetch evidence texts\n",
        "        evidence_texts = [self.evidence_data.get(e_id, \"\") for e_id in pred_evidences_ID]\n",
        "        evidence = \" [SEP] \".join(evidence_texts)\n",
        "\n",
        "        # Construct input text -- explicit marker with \"Claim\" and \"Evidence\"\n",
        "        inputs = self.tokenizer(\"CLAIM: \" + claim_text + \" [SEP] EVIDENCE: \" + evidence,\n",
        "                                truncation=True, padding='max_length',\n",
        "                                max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        inputs['labels'] = self.label_map[self.claim_data[claim_id]['claim_label']]\n",
        "        return inputs\n",
        "\n",
        "\n",
        "\n",
        "def create_dataloader(predicted_evidence_path, claim_path, evidence_path, tokenizer, batch_size=16, max_len=512):\n",
        "    dataset = DynamicEvidenceDataset(predicted_evidence_path, claim_path, evidence_path, tokenizer, max_len)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dzlophCRteq"
      },
      "source": [
        "Evidence Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "def predictions(cand_map: Dict[str, List[str]]) -> Dict[str, dict]:\n",
        "    return {cid: {\"evidences\": eids} for cid, eids in cand_map.items()} # convert to the expected predictions structure\n",
        "\n",
        "def evaluate(pred: dict, actual: dict):\n",
        "    # For each claim, get the set of gold and predicted evidence IDs\n",
        "    gold_sets = [set(actual[c][\"evidences\"]) for c in actual]\n",
        "    pred_sets = [set(pred.get(c, {}).get(\"evidences\", [])) for c in actual]\n",
        "\n",
        "    # Fit the label binarizer on the gold evidence universe\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    y_true = mlb.fit_transform(gold_sets)\n",
        "\n",
        "    valid_labels = set(mlb.classes_)  # only evidence IDs that appear in gold\n",
        "\n",
        "    # Filter predictions to only include valid evidence IDs (safe for shared use)\n",
        "    pred_sets_filtered = [\n",
        "        [eid for eid in preds if eid in valid_labels]\n",
        "        for preds in pred_sets\n",
        "    ]\n",
        "    y_pred = mlb.transform(pred_sets_filtered)\n",
        "\n",
        "    # Micro-averaged precision/recall/F1 (shared evidences per-claim are handled naturally)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"micro\", zero_division=0\n",
        "    )\n",
        "\n",
        "    return rec, prec, f1\n",
        "\n",
        "\n",
        "def faiss_candidates(\n",
        "    claims: Dict[str, dict], evidences: Dict[str, str],\n",
        "    model: SentenceTransformer, top_k: int\n",
        ") -> Tuple[List[str], List[str], Dict[str, List[str]]]:\n",
        "    claim_ids = list(claims)\n",
        "    claim_texts = [claims[cid][\"claim_text\"] for cid in claim_ids]\n",
        "    evidence_ids, evidence_texts = zip(*[(eid, txt) for eid, txt in evidences.items() if txt]) if evidences else ([], [])\n",
        "\n",
        "    emb_e = model.encode(evidence_texts, batch_size=64, show_progress_bar=True,\n",
        "                            convert_to_numpy=True, normalize_embeddings=True, device='cpu')\n",
        "\n",
        "    emb_c = model.encode(claim_texts, batch_size=64, show_progress_bar=True,\n",
        "                         convert_to_numpy=True, normalize_embeddings=True, device='cpu')\n",
        "\n",
        "    d = emb_e.shape[1]\n",
        "    index = faiss.IndexHNSWFlat(d, 32)\n",
        "    index.hnsw.efConstruction = 200\n",
        "\n",
        "    print(\"Adding embeddings to FAISS index in batches\")\n",
        "\n",
        "    total = len(emb_e)\n",
        "    progress_checkpoints = {int(total * i / 10) for i in range(1, 11)}  # 10%, 20%, ..., 100%\n",
        "\n",
        "    for i in range(0, total, 100):\n",
        "        try:\n",
        "            batch = emb_e[i:i+100]\n",
        "            index.add(batch)\n",
        "            current = i + len(batch)\n",
        "            if current in progress_checkpoints:\n",
        "                print(f\"{int(current / total * 100)}% complete ({current} / {total})\", flush=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed at batch {i}: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"Finished adding all vectors\")\n",
        "\n",
        "    print(\"Performing FAISS search (per claim)\")\n",
        "\n",
        "    I_all = []\n",
        "\n",
        "    for emb in tqdm(emb_c, desc=\"Searching claims\"):\n",
        "        sim_scores, I = index.search(np.expand_dims(emb, axis=0), top_k)\n",
        "        paired = list(zip(I[0], sim_scores[0]))\n",
        "\n",
        "        # Filter by similarity threshold\n",
        "        filtered = [idx for idx, score in paired if score >= SIMILARITY_THRESHOLD]\n",
        "\n",
        "        # If nothing passes threshold, take the best one as we want at-least 1 evidence per claim\n",
        "        if not filtered:\n",
        "            filtered = [paired[0][0]] if paired else []\n",
        "\n",
        "        I_all.append(filtered)\n",
        "\n",
        "    print(f\"Search done\")\n",
        "\n",
        "    # Create dict structure for the results\n",
        "    cand_map: Dict[str, List[str]] = {\n",
        "        claim_ids[i]: [\n",
        "            evidence_ids[j] for j in I_all[i] if j < len(evidence_ids)\n",
        "        ] for i in range(len(claim_ids))\n",
        "    }\n",
        "\n",
        "    return cand_map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZjm2xrTRvcK"
      },
      "source": [
        "Claim Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Yy7j_SvRyTJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import time\n",
        "\n",
        "# Run trained classification model on development set\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "\n",
        "            input_ids = batch['input_ids'].to('cuda')\n",
        "            attention_mask = batch['attention_mask'].to('cuda')\n",
        "            labels = batch['labels'].to('cuda')\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "\n",
        "    return all_preds, all_labels, inference_time\n",
        "\n",
        "\n",
        "# Evaluation pipeline\n",
        "def run_evaluation(predicted_evidence_path, claim_path, evidence_path, model, tokenizer,output_path, batch_size=16, max_len=512):\n",
        "\n",
        "    label_map = {0: 'SUPPORTS', 1: 'REFUTES', 2: 'NOT_ENOUGH_INFO', 3: 'DISPUTED'}\n",
        "\n",
        "    dataloader = create_dataloader(predicted_evidence_path, claim_path, evidence_path, tokenizer, batch_size, max_len)\n",
        "    preds, labels, inference_time = evaluate_model(model, dataloader)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "    # Prepare predicion output\n",
        "    output_data = {}\n",
        "    predicted_evidence_ID = json.load(open(predicted_evidence_path))\n",
        "\n",
        "    for idx, claim_id in enumerate(predicted_evidence_ID.keys()):\n",
        "        output_data[claim_id] = {\n",
        "            \"evidences\": predicted_evidence_ID[claim_id][\"evidences\"],\n",
        "            \"claim_label\": label_map[int(preds[idx])]\n",
        "        }\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(output_data, f, indent=4)\n",
        "\n",
        "    print(f\"Classification predictions saved to {output_path}\")\n",
        "    print(f\"Classification Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Classification Precision: {precision:.4f}\")\n",
        "    print(f\"Classification Recall: {recall:.4f}\")\n",
        "    print(f\"Classification F1 Score: {f1:.4f}\")\n",
        "    print(f\"Total Classification Inference Time: {inference_time:.2f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDB7jiRIROKk"
      },
      "source": [
        "Evidence Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "model = SentenceTransformer(FINETUNED_MODEL_PATH)\n",
        "\n",
        "dev_claims = load_claims(DEV_CLAIMS_PATH)\n",
        "evidences = load_evidences(EVIDENCE_PATH)\n",
        "\n",
        "cand_map = faiss_candidates(dev_claims, evidences, model, TOP_K)\n",
        "pred = predictions(cand_map) # Convert to appropriate predictions structure\n",
        "\n",
        "# save predictions file\n",
        "if PREDICTIONS_SAVE_PATH:\n",
        "    with open(PREDICTIONS_SAVE_PATH, \"w\") as f:\n",
        "        json.dump(pred, f, indent=2)\n",
        "    print(f\"Saved predictions to {PREDICTIONS_SAVE_PATH}\")\n",
        "\n",
        "recall, precision, f1 = evaluate(pred, dev_claims) # evaluate results\n",
        "print(f\"Recall: {recall:.4f}, Precision: {precision:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "'''\n",
        "Batches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18888/18888 [6:02:03<00:00,  4.86it/s]\n",
        "Batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.04it/s]\n",
        "Adding embeddings to FAISS index in batches\n",
        "10% complete (120882 / 1208827)\n",
        "20% complete (241765 / 1208827)\n",
        "30% complete (362648 / 1208827)\n",
        "40% complete (483530 / 1208827)\n",
        "50% complete (604413 / 1208827)\n",
        "60% complete (725296 / 1208827)\n",
        "70% complete (846179 / 1208827)\n",
        "80% complete (967062 / 1208827)\n",
        "90% complete (1087945 / 1208827)\n",
        "100% complete (1208827 / 1208827)\n",
        "Finished adding all vectors\n",
        "Performing FAISS search (per claim)\n",
        "Searching claims: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 154/154 [00:56<00:00,  2.75it/s]\n",
        "Search done\n",
        "Saved predictions to MyPredictions\n",
        "Recall: 0.0896, Precision: 0.1800, F1: 0.1176\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnqZtT1bRqsZ"
      },
      "source": [
        "Claim Classification with BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i0_24puQx-Z"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "STATE_DICT_PATH = \"Pretrained_model/new_bert_model_Autocast_explicitMarker_LR5e05.pt\"\n",
        "OUTPUT_PATH = \"Prediction/bert_prediction.json\"\n",
        "\n",
        "batch_size = 16\n",
        "max_len = 512\n",
        "\n",
        "# Load trained Bert Model and Tokenizer\n",
        "def load_model_and_tokenizer(state_dict_path):\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
        "    if state_dict_path:\n",
        "        state_dict = torch.load(state_dict_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "        print(\"successfully loaded finetuned BERT model\")\n",
        "    model.to('cuda')\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer(STATE_DICT_PATH)\n",
        "\n",
        "# Run classification evaluation with BERT Model\n",
        "run_evaluation(PREDICTIONS_SAVE_PATH, DEV_CLAIMS_PATH, EVIDENCE_PATH, model, tokenizer,OUTPUT_PATH, batch_size, max_len)\n",
        "\n",
        "'''\n",
        "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "successfully loaded finetuned BERT model\n",
        "Classification Predictions saved to Prediction/bert_prediction.json\n",
        "Classification Accuracy: 0.4610\n",
        "Classification Precision: 0.3464\n",
        "Classification Recall: 0.4610\n",
        "Classification F1 Score: 0.3806\n",
        "Total Classification Inference Time: 4.49 seconds\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KfSk6B-RBxx"
      },
      "source": [
        "Claim Classification with deBERTa-v3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ6a2VVxRBxx"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "STATE_DICT_PATH = \"Pretrained_model/deBERTa_v3_best_model.pt\"\n",
        "OUTPUT_PATH = \"Prediction/deBERTa_prediction.json\"\n",
        "\n",
        "batch_size = 4\n",
        "max_len = 512\n",
        "\n",
        "# Load trained deBERTa Model and Tokenizer\n",
        "def load_model_and_tokenizer(state_dict_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=4)\n",
        "    if state_dict_path:\n",
        "        state_dict = torch.load(state_dict_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "        print(\"successfully loaded finetuned deBERTa model\")\n",
        "    model.to('cuda')\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer(STATE_DICT_PATH)\n",
        "\n",
        "# Run classification evaluation with deBERTa Model\n",
        "run_evaluation(PREDICTIONS_SAVE_PATH, DEV_CLAIMS_PATH, EVIDENCE_PATH, model, tokenizer,OUTPUT_PATH, batch_size, max_len)\n",
        "\n",
        "'''\n",
        "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "successfully loaded finetuned deBERTa model\n",
        "Classification Predictions saved to Prediction/deBERTa_prediction.json\n",
        "Classification Accuracy: 0.4481\n",
        "Classification Precision: 0.3525\n",
        "Classification Recall: 0.4481\n",
        "Classification F1 Score: 0.3858\n",
        "Total Classification Inference Time: 7.49 seconds\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
